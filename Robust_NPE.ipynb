{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import zuko\n",
    "import lampe.inference, lampe.utils\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50000\n",
    "mu=0\n",
    "sigma = 0.01\n",
    "tau = 0.25\n",
    "rho = 1/2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(N, mu):\n",
    "    \n",
    "    sample = mu + torch.distributions.Normal(0, 1).sample((N, 100 ))\n",
    "    x_raw = torch.cat((sample.mean(dim=1, keepdim=True), sample.std(dim=1, keepdim=True)), dim=1)\n",
    "    \n",
    "    y_raw = x_raw + torch.distributions.Normal(0, 1).sample()\n",
    "    \n",
    "    return x_raw, y_raw\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_over_mu = torch.distributions.Normal(torch.zeros(1), torch.ones(1)*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulator(N_train,N_test, prior):\n",
    "    N = N_train + N_test\n",
    "    theta = prior.sample((N, ))\n",
    "    \n",
    "    sample = torch.distributions.Normal(theta.unsqueeze(1), 1.0).sample((100, ))\n",
    "    \n",
    "    x_raw = torch.cat((sample.mean(dim=0, keepdim=True), sample.std(dim=0, keepdim=True)), dim=2).squeeze_()\n",
    "    theta, theta_test = theta.split(N_train)\n",
    "    x_raw, x_raw_test = x_raw.split(N_train)\n",
    "    return theta, theta_test, x_raw, x_raw_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.8828e-09,  5.3040e-07])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, theta_test, x_raw, x_test_raw = simulator(N, N//10, prior_over_mu)\n",
    "\n",
    "x = (x_raw - x_raw.mean(0) ) / x_raw.std(0)\n",
    "x_test = (x_test_raw - x_test_raw.mean(0) ) / x_test_raw.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = lampe.data.JointDataset(theta, x)\n",
    "dataset_test = lampe.data.JointDataset(theta_test, x_test)\n",
    "_, y_raw = generate_data(N,5)\n",
    "y = (y_raw - y_raw.mean(0) ) / y_raw.std(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def true_dgp(mus):\n",
    "    \"\"\"True Data Generating Process. Generates samples from N(mu, 2)\n",
    "\n",
    "    Args:\n",
    "        mus (torch.Tensor): mean\n",
    "\n",
    "    Returns:\n",
    "        res (torch.Tensor): tensor of means, std of the samples\n",
    "    \"\"\"\n",
    "    res = torch.empty((mus.shape[0], 2))\n",
    "    for i, mu in enumerate(mus):\n",
    "        \n",
    "        dist=  torch.distributions.Normal(mu, 2)\n",
    "        sample = dist.sample((100, ))\n",
    "        mean, var = torch.mean(sample).item(), torch.var(sample).item()\n",
    "        x = mean, var\n",
    "        res[i] = torch.tensor(x)\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulation (line 1 & 2 of Algorithm 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def simulator(thetas: torch.Tensor):\n",
    "    \"\"\"Simulator, maps theta -> x = (mean, var)\n",
    "\n",
    "    Args:\n",
    "        thetas (torch.Tensor): Parameters\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of size (theta.size, 2) of (mean, var)\n",
    "    \"\"\"\n",
    "    N = thetas.size()[0]\n",
    "    x = torch.empty((N, 2))\n",
    "    for i, theta in enumerate(thetas):\n",
    "        dist = torch.distributions.Normal(theta, 1.0)\n",
    "        samples = dist.sample((100,))\n",
    "        means, var = torch.mean(samples), torch.var(samples)\n",
    "        x[i][0], x[i][1] = means, var\n",
    "    return x\n",
    "\n",
    "def scale(quantity):\n",
    "    '''\n",
    "    Standardizes the quantity\n",
    "    '''\n",
    "    means, std = quantity.mean(axis=0), quantity.std(axis=0)\n",
    "    quantity = quantity - means\n",
    "    quantity = quantity/std\n",
    "    return quantity\n",
    "def get_stats(quantity):\n",
    "    mean = quantity.mean(axis=0)\n",
    "    std = quantity.std(axis=0)\n",
    "    return mean, std\n",
    "\n",
    "stats = {}\n",
    "#Raw simulations\n",
    "theta = prior_over_mu.sample((N, ))\n",
    "stats[\"theta\"] = get_stats(theta)\n",
    "x_raw = simulator(theta)\n",
    "stats[\"x\"] = get_stats(x_raw)\n",
    "#Standardized versions\n",
    "\n",
    "x = scale(x_raw)\n",
    "dataset = lampe.data.JointDataset(theta, x)\n",
    "\n",
    "\n",
    "theta_test = scale(prior_over_mu.sample((N//10, )))\n",
    "x_test = scale(simulator(theta_test))\n",
    "dataset_test = lampe.data.JointDataset(theta_test, x_test)\n",
    "\n",
    "'''Creates a 'true' dataset (theta*, x*) ~ p(theta, x)'''\n",
    "\n",
    "\n",
    "thetas_star_raw = mu_dist.sample((N//10, ))\n",
    "stats[\"thetas_star\"] = get_stats(thetas_star_raw)\n",
    "thetas_star = scale(thetas_star_raw)\n",
    "y_star_raw = true_dgp(thetas_star)\n",
    "stats[\"y_star\"] = get_stats(y_star_raw)\n",
    "y_star = scale(y_star_raw)\n",
    "dataset_star = lampe.data.JointDataset(thetas_star.unsqueeze(1), y_star)\n",
    "\n",
    "def noisy_simulator():\n",
    "    \"\"\"Simulator taking the error model into account\n",
    "\n",
    "    Returns:\n",
    "        tuple: x_noisy, theta_noisy, x_noisy_test_raw, theta_noisy_test\n",
    "    \"\"\"\n",
    "    #Sample the parameters theta [mu]\n",
    "\n",
    "    x_noisy_raw, theta_noisy = x_raw.clone(), theta.clone()\n",
    "    x_noisy_test_raw, theta_noisy_test = x_test.clone(), theta_test.clone()\n",
    "\n",
    "    for i, x in enumerate(x_noisy_raw):\n",
    "        spike_dist = torch.distributions.Normal(x, sigma)\n",
    "        slab_dist = torch.distributions.Cauchy(x, tau)\n",
    "        spike = (1 - rho) *spike_dist.sample()\n",
    "        slab = rho * slab_dist.sample()\n",
    "        x_noisy_raw[i] += spike + slab\n",
    "    for i, x in enumerate(x_noisy_test_raw):\n",
    "        spike_dist = torch.distributions.Normal(x, torch.Tensor([sigma]))\n",
    "        slab_dist = torch.distributions.Cauchy(x, torch.Tensor([tau]))\n",
    "        spike = ((1 - rho) *spike_dist.sample((1, )))[0]\n",
    "        slab = (rho * slab_dist.sample((1, )))[0]\n",
    "        x_noisy_test_raw[i] += spike + slab\n",
    "    x_noisy = scale(x_noisy_raw)\n",
    "    return x_noisy, theta_noisy, x_noisy_test_raw, theta_noisy_test\n",
    "\n",
    "x_noisy, thetas_noisy, x_test_noisy, thetas_test_noisy = noisy_simulator()\n",
    "\n",
    "dataset_noisy = lampe.data.JointDataset(thetas_noisy, x_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NPE(flow, data, loss,theta_test, x_test, learning_rate=5*10e-4, max_epochs=50,batch_size=256,patience=5):\n",
    "    \"\"\"Trains a NPE.\n",
    "\n",
    "    Args:\n",
    "        flow (lampe.inference.NPE): A instanciated flow\n",
    "        data (lampe.data.JointDataset): dataset (theta, x)\n",
    "        loss (callable): a loss function\n",
    "        theta_test (torch.Tensor): test batch\n",
    "        x_test (torch.Tensor): test batch\n",
    "        learning_rate (float, optional): Defaults to 5*10e-4.\n",
    "        max_epochs (int, optional): Defaults to 50.\n",
    "        batch_size (int, optional): Defaults to 256.\n",
    "        patience (int, optional): Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        lampe.inference.NPE: the trained NPE\n",
    "    \"\"\"\n",
    "    \n",
    "    #Optim\n",
    "    optimizer = torch.optim.Adam(flow.parameters(), learning_rate)\n",
    "    step = lampe.utils.GDStep(optimizer)\n",
    "    #Creates the loader\n",
    "    loader = lampe.data.DataLoader(data, batch_size)\n",
    "    #For early stopping\n",
    "    with torch.no_grad():\n",
    "        min_loss = loss(theta_test,x_test)\n",
    "    min_loss_list = [min_loss]\n",
    "    \n",
    "    flow.train()\n",
    "    batch_norm_x = torch.nn.BatchNorm1d(x_test.shape[1])\n",
    "    batch_norm_theta = torch.nn.BatchNorm1d(theta_test.shape[1])\n",
    "    for epoch in tqdm.tqdm(range(max_epochs)):\n",
    "        \n",
    "        for theta_batch_raw, x_batch_raw in loader:\n",
    "            theta_batch = batch_norm_theta(theta_batch_raw)\n",
    "            x_batch = batch_norm_x(x_batch_raw)\n",
    "            losses = loss(theta_batch, x_batch)\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            batch_norm_theta(theta_batch_raw)\n",
    "            batch_norm_x(x_batch_raw)\n",
    "        \n",
    "        #Checking for early stopping\n",
    "        with torch.no_grad():\n",
    "            loss_test = loss(theta_test, x_test)\n",
    "            min_loss_list.append(loss_test)\n",
    "            if len(min_loss_list) - np.argmin(min_loss_list) > patience:\n",
    "                print(f\" Early stop at epoch {epoch}\")\n",
    "                break\n",
    "    flow.eval()\n",
    "    return flow\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unconditional(flow, x, x_test, loss, learning_rate=1e-2,max_epochs=50, batch_size=256, patience=5):\n",
    "    \"\"\"Train an unconditional flow\n",
    "\n",
    "    Args:\n",
    "        flow (zuko.flow): A instanciated flow\n",
    "        x (torch.Tensor): \n",
    "        x_test (torch.Tensor): test batch\n",
    "        loss (callable): a loss function\n",
    "        learning_rate (float, optional): Defaults to 1e-3.\n",
    "        max_epochs (int, optional): Defaults to 50.\n",
    "        batch_size (int, optional): Defaults to 256.\n",
    "        patience (int, optional): Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        zuko.flow: the trained flow\n",
    "    \"\"\"\n",
    "    \n",
    "    #Optim\n",
    "    optimizer = torch.optim.Adam(flow.parameters(), learning_rate)\n",
    "    \n",
    "    #Creates the loader\n",
    "    loader = torch.utils.data.DataLoader(x, batch_size)\n",
    "    min_loss_list = []\n",
    "    #Early stopping\n",
    "    with torch.no_grad():\n",
    "        min_loss = -loss(x_test)\n",
    "    min_loss_list = [min_loss]\n",
    "    \n",
    "    flow.train()\n",
    "    for epoch in tqdm.tqdm(range(max_epochs)):\n",
    "        \n",
    "        for x_batch in loader:\n",
    "                \n",
    "            losses = loss(x_batch)\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "        #Checking for early stopping\n",
    "        with torch.no_grad():\n",
    "            loss_test = loss(x_test)\n",
    "            min_loss_list.append(loss_test)\n",
    "            if len(min_loss_list) - np.argmin(min_loss_list) > patience:\n",
    "                print(f\"Early stop at epoch {epoch}\")\n",
    "                break\n",
    "    return flow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train NPE q(theta|x) on the simulated dataset {(thetai, xi)}, i=1, ... N\n",
    "Uses a neural spline flow defining the transform on the interval [-5, 5] using 10  spline segments and 5 coupling layers. The base of the flow is a standard Gaussian distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [00:26<05:08,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Early stop at epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset_noisy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m q_NPE_noisy \u001b[39m=\u001b[39m lampe\u001b[39m.\u001b[39minference\u001b[39m.\u001b[39mNPE(theta_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,x_dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,build\u001b[39m=\u001b[39mbuild_nsf)\n\u001b[1;32m      9\u001b[0m loss_NPE_noisy \u001b[39m=\u001b[39m lampe\u001b[39m.\u001b[39minference\u001b[39m.\u001b[39mNPELoss(q_NPE_noisy)\n\u001b[0;32m---> 10\u001b[0m q_NPE_noisy \u001b[39m=\u001b[39m train_NPE(q_NPE_noisy, dataset_noisy, loss_NPE_noisy, thetas_test_noisy,x_test_noisy)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_noisy' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_nsf(features, context):\n",
    "    \"\"\"Callable to instanciate the NPE with NSFs\"\"\"\n",
    "    return zuko.flows.NSF(features, context, bins=10, transforms=5)\n",
    "q_NPE = lampe.inference.NPE(theta_dim = 1, x_dim=2, build=build_nsf)\n",
    "loss_NPE = lampe.inference.NPELoss(q_NPE)\n",
    "q_NPE = train_NPE(q_NPE, dataset, loss_NPE, theta_test, x_test)\n",
    "'''\n",
    "q_NPE_noisy = lampe.inference.NPE(theta_dim=1,x_dim=2,build=build_nsf)\n",
    "loss_NPE_noisy = lampe.inference.NPELoss(q_NPE_noisy)\n",
    "q_NPE_noisy = train_NPE(q_NPE_noisy, dataset_noisy, loss_NPE_noisy, thetas_test_noisy,x_test_noisy)\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling theta here as in the 'classic' NPE framework (i.e. assuming no error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas_NPE = q_NPE.sample(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train q(x) on {xi} i=1, ... N (so first gen the x)\n",
    "For q(x) uses of a block neural autoregressive flow, single hidden layer of size 8D, x in R^d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [00:09<01:46,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stop at epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "q_x_NF = zuko.flows.NAF(features = 2, context=0, hidden_features=[8]*2,transforms=1)\n",
    "loss_q_x = lambda x: -q_x_NF().log_prob(x).mean()\n",
    "q_x_NF = train_unconditional(q_x_NF, x, x_test, loss_q_x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample x~m from p(x | y0) % p(y0 \\ x) q(x), m = 1, ... M using MCMC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spike and slab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 100000\n",
    "warm_up_steps = 20000\n",
    "traj_length = 1\n",
    "target_acceptance_prob = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHQTransition(lampe.inference.MetropolisHastings):\n",
    "    def q(self, x:torch.Tensor):\n",
    "        return q_x_NF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we observe y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.0964, 3.0077])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y0 = y[0]\n",
    "y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_y_given_x(x):\n",
    "    z = torch.distributions.Bernoulli(rho).sample(x.shape)\n",
    "    res = 1\n",
    "    for j,x_j in enumerate(x):\n",
    "        zj=z[j]\n",
    "        if not zj:\n",
    "            dist=  torch.distributions.Normal(x_j, sigma)\n",
    "        else:\n",
    "            dist = torch.distributions.Cauchy(x_j, tau)\n",
    "        res *= dist.log_prob(y0).exp().sum()\n",
    "    return res\n",
    "\n",
    "def f(x):\n",
    "    qx = q_x_NF().log_prob(x).exp()\n",
    "    pyx = p_y_given_x(x)\n",
    "    return qx*pyx           \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100000it [04:27, 373.38it/s]\n"
     ]
    }
   ],
   "source": [
    "sampler = lampe.inference.MetropolisHastings(q_x_NF().sample(),f=f)\n",
    "with torch.no_grad():\n",
    "    xm_samples = [s for s in tqdm.tqdm(sampler(M+warm_up_steps,burn=warm_up_steps))]\n",
    "    xm_samples = torch.stack(xm_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "def compare(flow,x,file):\n",
    "    class1 = file\n",
    "    post = flow.flow(x).sample()\n",
    "    #post = scale(post)\n",
    "    post_array = post.detach().numpy()\n",
    "    class0 = post_array\n",
    "    class0_labels = np.zeros(len(class0))\n",
    "    class1_labels = np.ones(len(class1))\n",
    "    data = np.concatenate((class0,class1), axis=0)\n",
    "    labels = np.concatenate((class0_labels, class1_labels), axis=0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data,labels)\n",
    "\n",
    "    clf = QuadraticDiscriminantAnalysis()\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = sklearn.metrics.accuracy_score(y_test,y_pred)\n",
    "    return acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9644"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = np.load(\"/home/tux/rnpe/robust_samples_paper.npy\", allow_pickle=True)\n",
    "compare(q_NPE,xm_samples,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y0 = y0.to(device)\n",
    "y0.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/120000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m log_prob_accept \u001b[39m=\u001b[39m log_prob_proposal \u001b[39m-\u001b[39m log_prob_current\n\u001b[1;32m     35\u001b[0m \u001b[39m# Accept or reject the proposal\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[39mif\u001b[39;00m log_prob_accept \u001b[39m>\u001b[39;49m torch\u001b[39m.\u001b[39;49mlog(torch\u001b[39m.\u001b[39;49mrand(\u001b[39m1\u001b[39;49m)):\n\u001b[1;32m     37\u001b[0m     x_current \u001b[39m=\u001b[39m x_proposal\n\u001b[1;32m     38\u001b[0m x_samples\u001b[39m.\u001b[39mappend(x_current\u001b[39m.\u001b[39mclone())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "# Define the log-posterior distribution\n",
    "D = x.shape[1]\n",
    "def log_posterior(x, y):\n",
    "    z = torch.distributions.Bernoulli(rho).sample(x.shape)\n",
    "    log_prob = 0\n",
    "    for j, x_j in enumerate(x):\n",
    "        z_j = z[j]\n",
    "        if not z_j:\n",
    "            dist = torch.distributions.Normal(x_j, 0.01)\n",
    "        else:\n",
    "            dist = torch.distributions.Cauchy(x_j, 0.25)\n",
    "        log_prob += dist.log_prob(y).sum()\n",
    "    return log_prob\n",
    "\n",
    "# Define the proposal distribution\n",
    "def proposal(x):\n",
    "    return torch.distributions.Normal(x, 0.1).sample()\n",
    "\n",
    "# Set the initial state and number of iterations\n",
    "x_init = torch.zeros(D)\n",
    "n_iter = M+warm_up_steps\n",
    "\n",
    "# Run the Metropolis-Hastings algorithm\n",
    "x_samples = []\n",
    "x_current = x_init.clone()\n",
    "for i in tqdm.tqdm(range(n_iter)):\n",
    "    # Propose a new state\n",
    "    x_proposal = proposal(x_current)\n",
    "    \n",
    "    # Compute the acceptance probability\n",
    "    log_prob_current = log_posterior(x_current, y0)\n",
    "    log_prob_proposal = log_posterior(x_proposal, y0)\n",
    "    log_prob_accept = log_prob_proposal - log_prob_current\n",
    "    \n",
    "    # Accept or reject the proposal\n",
    "    if log_prob_accept > torch.log(torch.rand(1)):\n",
    "        x_current = x_proposal\n",
    "    x_samples.append(x_current.clone())\n",
    "\n",
    "# Convert the samples to a tensor\n",
    "x_samples = x_samples[warm_up_steps:len(x_samples)]\n",
    "x_samples = torch.stack(x_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999272727272728"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(q_NPE,x_samples,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9114181818181818"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_xm = np.load(\"/home/tux/rnpe/denoise_xm_paper.npy\")\n",
    "xm_tensor = torch.Tensor(file_xm)\n",
    "compare(q_NPE, xm_tensor, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50132"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_NPE = np.load(\"/home/tux/rnpe/thetas_NPE.npy\")\n",
    "compare(q_NPE, x, file_NPE) # q(theta | x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93654"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_xm(my_xm, xm_paper):\n",
    "    class1 = xm_paper\n",
    "    class0 = my_xm.detach().numpy()\n",
    "    class0_labels = np.zeros(len(class0))\n",
    "    class1_labels = np.ones(len(class1))\n",
    "    data = np.concatenate((class0,class1), axis=0)\n",
    "    labels = np.concatenate((class0_labels, class1_labels), axis=0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data,labels)\n",
    "\n",
    "    clf = LinearDiscriminantAnalysis()\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = sklearn.metrics.accuracy_score(y_test,y_pred)\n",
    "    return acc\n",
    "\n",
    "compare_xm(xm_samples,file_xm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample theta ~q(theta | xm), m = 1, ..., M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    thetasm = q_NPE.sample(xm_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_star' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlampe\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdiagnostics\u001b[39;00m \u001b[39mimport\u001b[39;00m expected_coverage_mc, expected_coverage_ni\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlampe\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplots\u001b[39;00m \u001b[39mimport\u001b[39;00m nice_rc, coverage_plot\n\u001b[0;32m----> 3\u001b[0m npe_levels, npe_coverages \u001b[39m=\u001b[39m expected_coverage_mc(q_NPE\u001b[39m.\u001b[39mflow, dataset_star)\n\u001b[1;32m      4\u001b[0m nnpe_levels, nnpe_coverages \u001b[39m=\u001b[39m expected_coverage_mc(q_NPE_noisy\u001b[39m.\u001b[39mflow, dataset_star)\n\u001b[1;32m      6\u001b[0m fig \u001b[39m=\u001b[39m coverage_plot(npe_levels, npe_coverages, legend\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNPE\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_star' is not defined"
     ]
    }
   ],
   "source": [
    "from lampe.diagnostics import expected_coverage_mc, expected_coverage_ni\n",
    "from lampe.plots import nice_rc, coverage_plot\n",
    "npe_levels, npe_coverages = expected_coverage_mc(q_NPE.flow, dataset_star)\n",
    "nnpe_levels, nnpe_coverages = expected_coverage_mc(q_NPE_noisy.flow, dataset_star)\n",
    "\n",
    "fig = coverage_plot(npe_levels, npe_coverages, legend='NPE')\n",
    "fig = coverage_plot(nnpe_levels, nnpe_coverages, legend='NNPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_prob(flow, thetas, x):\n",
    "    res = []\n",
    "    for i, xi in enumerate(x):\n",
    "        log_prob = flow.flow(xi).log_prob(thetas)\n",
    "        res.append(log_prob.detach().item())\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x5002 and 3x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m log_probs_RNPE \u001b[39m=\u001b[39m compute_log_prob(q_NPE, thetas_star, xm_samples)\n\u001b[1;32m      2\u001b[0m log_probs_NPE \u001b[39m=\u001b[39m compute_log_prob(q_NPE, thetas_star, x)\n\u001b[1;32m      3\u001b[0m log_probs_NNPE \u001b[39m=\u001b[39m compute_log_prob(q_NPE_noisy, thetas_star, x_noisy)\n",
      "Cell \u001b[0;32mIn[83], line 4\u001b[0m, in \u001b[0;36mcompute_log_prob\u001b[0;34m(flow, thetas, x)\u001b[0m\n\u001b[1;32m      2\u001b[0m res \u001b[39m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m i, xi \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(x):\n\u001b[0;32m----> 4\u001b[0m     log_prob \u001b[39m=\u001b[39m flow\u001b[39m.\u001b[39;49mflow(xi)\u001b[39m.\u001b[39;49mlog_prob(thetas)\n\u001b[1;32m      5\u001b[0m     res\u001b[39m.\u001b[39mappend(log_prob\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mitem())\n\u001b[1;32m      6\u001b[0m \u001b[39mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/miniconda3/envs/rnpe_env/lib/python3.9/site-packages/zuko/distributions.py:110\u001b[0m, in \u001b[0;36mNormalizingFlow.log_prob\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog_prob\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 110\u001b[0m     z, ladj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform\u001b[39m.\u001b[39;49mcall_and_ladj(x)\n\u001b[1;32m    111\u001b[0m     ladj \u001b[39m=\u001b[39m _sum_rightmost(ladj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreinterpreted)\n\u001b[1;32m    113\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase\u001b[39m.\u001b[39mlog_prob(z) \u001b[39m+\u001b[39m ladj\n",
      "File \u001b[0;32m~/miniconda3/envs/rnpe_env/lib/python3.9/site-packages/zuko/transforms.py:136\u001b[0m, in \u001b[0;36mComposedTransform.call_and_ladj\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    133\u001b[0m acc \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    135\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m--> 136\u001b[0m     x, ladj \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mcall_and_ladj(x)\n\u001b[1;32m    137\u001b[0m     acc \u001b[39m=\u001b[39m acc \u001b[39m+\u001b[39m _sum_rightmost(ladj, event_dim \u001b[39m-\u001b[39m t\u001b[39m.\u001b[39mdomain\u001b[39m.\u001b[39mevent_dim)\n\u001b[1;32m    138\u001b[0m     event_dim \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mcodomain\u001b[39m.\u001b[39mevent_dim \u001b[39m-\u001b[39m t\u001b[39m.\u001b[39mdomain\u001b[39m.\u001b[39mevent_dim\n",
      "File \u001b[0;32m~/miniconda3/envs/rnpe_env/lib/python3.9/site-packages/zuko/transforms.py:725\u001b[0m, in \u001b[0;36mAutoregressiveTransform.call_and_ladj\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_and_ladj\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Tensor, Tensor]:\n\u001b[0;32m--> 725\u001b[0m     y, ladj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmeta(x)\u001b[39m.\u001b[39mcall_and_ladj(x)\n\u001b[1;32m    726\u001b[0m     \u001b[39mreturn\u001b[39;00m y, ladj\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rnpe_env/lib/python3.9/site-packages/zuko/flows.py:310\u001b[0m, in \u001b[0;36mMaskedAutoregressiveTransform.meta\u001b[0;34m(self, y, x)\u001b[0m\n\u001b[1;32m    306\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(broadcast(x, y, ignore\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    308\u001b[0m total \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msizes)\n\u001b[0;32m--> 310\u001b[0m phi \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhyper(x)\n\u001b[1;32m    311\u001b[0m phi \u001b[39m=\u001b[39m phi\u001b[39m.\u001b[39munflatten(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, (phi\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m total, total))\n\u001b[1;32m    312\u001b[0m phi \u001b[39m=\u001b[39m phi\u001b[39m.\u001b[39msplit(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msizes, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rnpe_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/rnpe_env/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/rnpe_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/rnpe_env/lib/python3.9/site-packages/zuko/nn.py:127\u001b[0m, in \u001b[0;36mMaskedLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmask \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x5002 and 3x64)"
     ]
    }
   ],
   "source": [
    "log_probs_RNPE = compute_log_prob(q_NPE, thetas_star, xm_samples)\n",
    "log_probs_NPE = compute_log_prob(q_NPE, thetas_star, x)\n",
    "log_probs_NNPE = compute_log_prob(q_NPE_noisy, thetas_star, x_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Axes' object has no attribute 'set_ylabels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[208], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m ax\u001b[39m.\u001b[39mboxplot([log_probs_RNPE, log_probs_NPE, log_probs_NNPE])\n\u001b[1;32m      3\u001b[0m ax\u001b[39m.\u001b[39mset_xticklabels([\u001b[39m\"\u001b[39m\u001b[39mlog-probs RNPE\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mlog-probs NPE\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlog-probs NNPE\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m----> 4\u001b[0m ax\u001b[39m.\u001b[39;49mset_ylabels(\u001b[39m\"\u001b[39m\u001b[39mLog-probs\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Axes' object has no attribute 'set_ylabels'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.boxplot([log_probs_RNPE, log_probs_NPE, log_probs_NNPE])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut aussi faire les 2 autres simu cad NPE (comme avant) et en mettant directement le modele d'erreur dans le simu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnpe_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
