{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lampe\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import zuko\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import scipy.integrate as integrate\n",
    "\n",
    "from IPython.display import display\n",
    "import pickle\n",
    "from typing import Tuple, Callable, List, Dict\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100  # Number of simulations before taking a summary\n",
    "theta_grid = np.linspace(-8, 8, 1000)\n",
    "N_simu = 50_000  # Number of simulations\n",
    "M = 100_000  # Number of samples from the MCMC algorithm\n",
    "warm_up_steps = 20_000  # burn in period\n",
    "sigma = 0.01  # In the error model, std of the spikes\n",
    "tau = 0.25  # In the erorr model, parameter of the Cauchy distribution\n",
    "rho = 1 / 2  # In the error model, parameter of the Bernoulli distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_summaries(thetas: Tuple, x1: float, x2: float, sigma_2_y: float = 1):\n",
    "    r\"\"\"Compute the true posterior of :math:`p(\\theta | x)`.\n",
    "\n",
    "    Args:\n",
    "        thetas: points where we want the density function to be computed.\n",
    "        x1: First dimension of the observation.\n",
    "        x2: Second dimension of the observation.\n",
    "        sigma_2_y (optional): Corruption. Defaults to 1 (no corruption).\n",
    "    \"\"\"\n",
    "\n",
    "    def improper_posterior_summaries(theta, x1=x1, x2=x2):\n",
    "        likelihood_x1_part = -N * np.square(theta - x1) / (2 * sigma_2_y**2)\n",
    "        likelihood_x2_part = (\n",
    "            -0.5 * np.square(sigma_2_y - x2) / (2 * np.square(sigma_2_y) / N)\n",
    "        )\n",
    "        prior_part = -np.square(theta) / 50\n",
    "        return (\n",
    "            np.exp(likelihood_x1_part) * np.exp(likelihood_x2_part) * np.exp(prior_part)\n",
    "        )\n",
    "\n",
    "    z_summaries, eps = integrate.quad(improper_posterior_summaries, -25, 25)\n",
    "    return [\n",
    "        improper_posterior_summaries(theta, x1, x2) / z_summaries for theta in thetas\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CS:\n",
    "    r\"\"\"Cancer and Stromal cell development in 2D space, marked point process.\n",
    "    Total number of cells :math:`N^c`, the number of unobserved parents :math:`N^p` and the nomber of daughters for each parent :math:`N_i^d`\n",
    "    :math:`N^c \\sim Poisson(\\lambda^c)`\n",
    "    :ath:`N^p \\sim Poisson(\\lambda^p)\n",
    "    :math:`N^d_i \\sim Poisson(\\lamda^d), i = 1,\\ldots,N^p\n",
    "    where :math:`\\lambda^c,\\lambda^p,\\lamda^d` are the parameters.\n",
    "    :math:`The affected radius r_i` represents the Euclidian distance from parent :math:`p_i` to its :math:`N_i^d`th nearest cell. Cells within this radius are infected.\n",
    "\n",
    "    Data are summaried in a summary statisics x = N_cancer, N stromal, Mean Min Dist, Max Min Dist (Mean min/max distance from stromal cell to their nearest cancer cell).\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, N_simu: int, miss_specification_param: float = 0.75, N_obs: int = M\n",
    "    ) -> None:\n",
    "        \"\"\"Instanciate the object, runs the simulation and the generate the actual observed data.\n",
    "\n",
    "        Args:\n",
    "            N_simu: Number of simulations\n",
    "            miss_specification_param (optional): Parameter of the Bernoulli distribution. Defaults to 0.75.\n",
    "            N_obs (optional): Number of real observation to generate. Defaults to M.\n",
    "        \"\"\"\n",
    "        self.name = \"CS\"\n",
    "        self.prior_c = torch.distributions.Uniform(low=200, high=1500)\n",
    "        self.prior_p = torch.distributions.Uniform(low=3, high=20)\n",
    "        self.prior_d = torch.distributions.Uniform(low=10, high=20)\n",
    "        self.has_post = False\n",
    "        rate_c_0, rate_p_0, rate_d_0 = self.prior_sample()\n",
    "        self.theta0 = torch.tensor([rate_c_0, rate_p_0, rate_d_0]).to(device)\n",
    "        self.miss_specification_param = miss_specification_param\n",
    "\n",
    "        self.features_dim = 4\n",
    "        self.parameters_dim = 3  # self.simulate_data_batch(64, N_simu)\n",
    "        self.data, self.true_obs_data = self.simulate_data(N_simu)\n",
    "        self.data_test, _ = self.simulate_data(N_simu // 10)\n",
    "        # self.true_obs_data = self.gen_obs(N_obs, self.theta0)\n",
    "        self.y0 = self.true_obs_data[\"y\"][0]\n",
    "        self.y0_l = self.true_obs_data[\"y\"].squeeze()\n",
    "        self.y0_scaled = (self.y0 - self.data[\"scale_parameters\"][0]) / self.data[\n",
    "            \"scale_parameters\"\n",
    "        ][1]\n",
    "        self.y0_l_scaled = (self.y0_l - self.data[\"scale_parameters\"][0]) / self.data[\n",
    "            \"scale_parameters\"\n",
    "        ][1]\n",
    "\n",
    "    def prior_sample(self, n: int = 1) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "        \"\"\"Samples from the priors\n",
    "\n",
    "        Args:\n",
    "            n (optional): Number of samples. Defaults to 1.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor, Tensor]: Samples from the 3 priors\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.prior_c.sample((n,)).to(device),\n",
    "            self.prior_p.sample((n,)).to(device),\n",
    "            self.prior_d.sample((n,)).to(device),\n",
    "        )\n",
    "\n",
    "    def simulate_data(self, N_simu: int, theta0: int = None) -> Tuple[Dict, Dict]:\n",
    "        \"\"\"Simulates the data, and generates the true observed one.\n",
    "\n",
    "        Args:\n",
    "            N_simu: Number of simulations\n",
    "            theta0 (optional): Theta used as reference for the observed data. Vector containing the values of :math:`\\lambda^c, \\lambda^p, \\lambda^d`. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Dict, Dict]: _description_\n",
    "        \"\"\"\n",
    "\n",
    "        simu_data = {\n",
    "            \"theta\": [],\n",
    "            \"x\": [],\n",
    "            \"cell_positions\": [],\n",
    "            \"parent_positions\": [],\n",
    "            \"cell_types\": [],\n",
    "        }\n",
    "        true_obs = {\n",
    "            \"theta\": [],\n",
    "            \"y\": [],\n",
    "            \"cell_positions_obs\": [],\n",
    "            \"parent_positions_obs\": [],\n",
    "            \"cell_types_obs\": [],\n",
    "        }\n",
    "\n",
    "        pbar = tqdm(total=N_simu, desc=\"Simulating data\", unit=\"data\")  # Progress bar\n",
    "        n_simulated = 0\n",
    "        while n_simulated < N_simu:\n",
    "            if theta0 is not None:\n",
    "                rate_c, rate_p, rate_d = theta0\n",
    "            else:\n",
    "                # Sample from the prior #TODO Prendre le sef.prior\n",
    "                rate_c, rate_p, rate_d = self.prior_sample()\n",
    "                rate_c.to(device)\n",
    "                rate_p.to(device)\n",
    "                rate_d.to(device)\n",
    "\n",
    "            N_c = int(\n",
    "                torch.distributions.Poisson(rate=rate_c).sample().item()\n",
    "            )  # Number of cells\n",
    "            N_p = int(\n",
    "                torch.distributions.Poisson(rate=rate_p).sample().item()\n",
    "            )  # Number of parents\n",
    "            N_d = (\n",
    "                torch.distributions.Poisson(rate=rate_d).sample((N_c,)).to(device)\n",
    "            )  # Number of daughters per parents\n",
    "\n",
    "            c = (\n",
    "                torch.distributions.Uniform(low=0, high=1).sample((N_c, 2)).to(device)\n",
    "            )  # Cells\n",
    "            c_obs = (\n",
    "                torch.distributions.Uniform(low=0, high=1).sample((N_c, 2)).to(device)\n",
    "            )\n",
    "            p = (\n",
    "                torch.distributions.Uniform(low=0, high=1).sample((N_p, 2)).to(device)\n",
    "            )  # Parents\n",
    "\n",
    "            cell_types = torch.zeros(\n",
    "                N_c, dtype=torch.bool, device=device\n",
    "            )  # Stromal or not\n",
    "            cell_types_obs = torch.zeros(N_c, dtype=torch.bool, device=device)\n",
    "            for parent_idx in range(N_p):\n",
    "                parent = p[parent_idx]\n",
    "\n",
    "                num_daughters = int(N_d[parent_idx].item())\n",
    "                num_available_daughters = c.shape[0]\n",
    "\n",
    "                if num_daughters > num_available_daughters:\n",
    "                    num_daughters = num_available_daughters\n",
    "\n",
    "                _, daughters_idx = torch.topk(\n",
    "                    torch.cdist(c, parent.unsqueeze(0), p=2).squeeze(),\n",
    "                    num_daughters,\n",
    "                    largest=False,\n",
    "                )\n",
    "                daughters = c[daughters_idx]\n",
    "                r = (\n",
    "                    torch.cdist(daughters, parent.unsqueeze_(0), p=2).max().item()\n",
    "                    if daughters.numel() != 0\n",
    "                    else 0\n",
    "                )\n",
    "\n",
    "                distances = torch.cdist(c, parent.unsqueeze(0), p=2).squeeze()\n",
    "                cell_types[distances <= r] = 1\n",
    "                cell_types_obs = cell_types.clone()\n",
    "\n",
    "            if torch.distributions.Bernoulli(self.miss_specification_param).sample():\n",
    "                # Miss specification\n",
    "                removal_radius = r * 0.8\n",
    "\n",
    "                if (\n",
    "                    cell_types.shape == distances.shape\n",
    "                ):  # Check if the mask shape matches cell_types shape\n",
    "                    mask = (cell_types == 1) & (distances <= removal_radius)\n",
    "                    c_obs = c[~mask]\n",
    "                    cell_types_obs = cell_types[~mask]\n",
    "\n",
    "            n_cancer_cells = torch.sum(cell_types).item()\n",
    "            n_stromal_cells = N_c - n_cancer_cells\n",
    "            stromal_cells = c[cell_types == 0]\n",
    "            cancer_cells = c[cell_types == 1]\n",
    "\n",
    "            # Summarizing\n",
    "            if len(stromal_cells) > 0 and len(cancer_cells) > 0:\n",
    "                min_distances = (\n",
    "                    torch.cdist(stromal_cells, cancer_cells, p=2).min(dim=1).values\n",
    "                )\n",
    "                mean_min_dist = torch.mean(min_distances).item()\n",
    "                max_min_dist = torch.max(min_distances).item()\n",
    "\n",
    "            n_cancer_cells_obs = torch.sum(cell_types_obs).item()\n",
    "            n_stromal_cells_obs = N_c - n_cancer_cells_obs\n",
    "            stromal_cells_obs = c_obs[cell_types_obs]\n",
    "            cancer_cells_obs = c_obs[cell_types_obs]\n",
    "            if len(stromal_cells_obs) > 0 and len(cancer_cells_obs) > 0:\n",
    "                min_distances_obs = (\n",
    "                    torch.cdist(stromal_cells_obs, cancer_cells_obs, p=2)\n",
    "                    .min(dim=1)\n",
    "                    .values\n",
    "                )\n",
    "                mean_min_dist_obs = torch.mean(min_distances_obs).item()\n",
    "                max_min_dist_obs = torch.max(min_distances_obs).item()\n",
    "\n",
    "            if (\n",
    "                len(stromal_cells_obs) > 0\n",
    "                and len(cancer_cells_obs) > 0\n",
    "                and len(stromal_cells) > 0\n",
    "                and len(cancer_cells) > 0\n",
    "            ):\n",
    "                theta = torch.tensor([rate_c, rate_p, rate_d])\n",
    "                x = torch.tensor(\n",
    "                    [n_cancer_cells, n_stromal_cells, mean_min_dist, max_min_dist]\n",
    "                )\n",
    "                simu_data[\"theta\"].append(theta)\n",
    "                true_obs[\"theta\"].append(theta)\n",
    "                simu_data[\"x\"].append(x)\n",
    "                simu_data[\"cell_positions\"].append(c)\n",
    "                simu_data[\"parent_positions\"].append(p)\n",
    "                simu_data[\"cell_types\"].append(cell_types)\n",
    "\n",
    "                y = torch.tensor(\n",
    "                    [\n",
    "                        n_cancer_cells_obs,\n",
    "                        n_stromal_cells_obs,\n",
    "                        mean_min_dist_obs,\n",
    "                        max_min_dist_obs,\n",
    "                    ]\n",
    "                )\n",
    "                true_obs[\"y\"].append(y)\n",
    "                true_obs[\"cell_positions_obs\"].append(c)\n",
    "                true_obs[\"parent_positions_obs\"].append(p)\n",
    "                true_obs[\"cell_types_obs\"].append(cell_types_obs)\n",
    "\n",
    "                n_simulated += 1\n",
    "                pbar.update(1)\n",
    "        true_obs[\"cell_positions_obs\"] = true_obs[\"cell_positions_obs\"][0]\n",
    "        true_obs[\"parent_positions_obs\"] = true_obs[\"parent_positions_obs\"][0]\n",
    "        true_obs[\"cell_types_obs\"] = true_obs[\"cell_types_obs\"][0]\n",
    "        true_obs[\"theta\"] = torch.stack(true_obs[\"theta\"])\n",
    "        simu_data[\"theta\"] = torch.stack(simu_data[\"theta\"])\n",
    "        true_obs[\"y\"] = torch.stack(true_obs[\"y\"])\n",
    "        simu_data[\"x\"] = torch.stack(simu_data[\"x\"])\n",
    "        scale_mean, scale_std = simu_data[\"x\"].mean(0), simu_data[\"x\"].std(0)\n",
    "        simu_data[\"scale_parameters\"] = scale_mean, scale_std\n",
    "        simu_data[\"scaled_x\"] = (simu_data[\"x\"] - scale_mean) / scale_std\n",
    "        # simu_obs_data[\"y\"]=  (simu_data[\"y\"] - scale_mean) / scale_std\n",
    "        # simu_data[\"scaled_y\"] = (simu_data[\"y\"] - scale_mean) / scale_std\n",
    "        pbar.close()\n",
    "        return (simu_data, true_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gaussian:\n",
    "    r\"\"\"Gaussian Task.\n",
    "        - Simulation: :math:`z_i \\sim \\mathcal{N}(\\theta, 1), i=1,...N_simu`\n",
    "        - TDGP: :math:`z_i \\sim \\mathcal{N}(\\theta, \\sigma^2), i=1,...N_simu`\n",
    "        - Feature space: Summary statistic :math:`x=\\left(mean(z_1,...z_{N_simu}); var(z_1,...z_{N_simu)\\right)`\n",
    "    NB: We can see the TDGP such that :math:`y = x + \\epsiolon, x \\sim \\mathcal{M}(\\theta), \\epsilon \\sim \\mathcal{N}(0, \\sigma_y^2).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, N_simu: int, sigma_y_2: float, true_theta: Tensor = torch.zeros(1)\n",
    "    ) -> None:\n",
    "        r\"\"\"Init the object, including datasets.\n",
    "\n",
    "        Args:\n",
    "            N_simu: total number of simulations\n",
    "            sigma_y_2: Corruption, Var of the observed data.\n",
    "        \"\"\"\n",
    "        self.name = \"Gaussian\"\n",
    "        self.prior = torch.distributions.Normal(0, 5)\n",
    "        self.sigma_y_2 = sigma_y_2\n",
    "        self.features_dim = 2\n",
    "        self.parameters_dim = 1\n",
    "        self.true_theta = true_theta\n",
    "        self.has_post = True\n",
    "        data = self.generate_data(N_simu, sigma_y_2)\n",
    "        data_test = self.generate_data(N_simu // 10, sigma_y_2)\n",
    "        true_obs_data = self.generate_data(10_000, sigma_y_2, true_theta)\n",
    "\n",
    "        x0 = true_obs_data[\"x\"][0]\n",
    "        y0 = x0 + true_obs_data[\"eps\"][0]\n",
    "        x0_scaled = (x0 - data[\"scale_parameters\"][0]) / data[\"scale_parameters\"][1]\n",
    "        y0_scaled = (y0 - data[\"scale_parameters\"][0]) / data[\"scale_parameters\"][1]\n",
    "        self.y0_l = true_obs_data[\"x\"] + true_obs_data[\"eps\"]\n",
    "        self.data = data\n",
    "        self.data_test = data_test\n",
    "        self.x0 = x0.squeeze()\n",
    "        self.y0 = y0.squeeze()\n",
    "        self.x0_scaled = x0_scaled\n",
    "        self.y0_scaled = y0_scaled.squeeze()\n",
    "        self.y0_l_scaled = (self.y0_l - data[\"scale_parameters\"][0]) / data[\n",
    "            \"scale_parameters\"\n",
    "        ][1]\n",
    "\n",
    "    def prior_sample(self, n):\n",
    "        return self.prior.sample((n, 1))\n",
    "\n",
    "    def generate_data(self, N: int, sigma_y_2: float = 1, thetas: Tensor = None):\n",
    "        \"\"\"Generates data according to #TODO Maths ici\n",
    "\n",
    "        Args:\n",
    "            N: Total number of simulations\n",
    "            sigma_y_2 (optional): Corruption. Defaults to 1.\n",
    "            theta (optional): Reference theta.\n",
    "\n",
    "        Returns:\n",
    "            res: dictionnary whose entries are parameters thetas, the raw simulations x, the corruption espilons as well as the scaled observations and the scale parameters.\n",
    "        \"\"\"\n",
    "        if thetas is None:\n",
    "            thetas = self.prior_sample(N)\n",
    "        sigma_eps = (\n",
    "            sigma_y_2 - 1\n",
    "        )  # If the TDGP has a variance of \\sigma^2, the corruption layer has \\sigma-1\n",
    "        res = {}\n",
    "        res[\"theta\"] = thetas\n",
    "        means_and_vars = torch.zeros((N, 2), device=device)\n",
    "\n",
    "        for i, theta in enumerate(thetas):\n",
    "            z = torch.distributions.Normal(theta, 1).sample((100,)).to(device)\n",
    "            mean = z.mean()\n",
    "            var = z.var()\n",
    "            means_and_vars[i] = torch.stack(\n",
    "                [mean.unsqueeze_(0), var.unsqueeze_(0)]\n",
    "            ).squeeze_()\n",
    "\n",
    "        res[\"x\"] = means_and_vars\n",
    "\n",
    "        norm_max = -1\n",
    "        for n_run in range(1):\n",
    "            # We take the run where the corruption has more effect, to be able to compare.\n",
    "            eps = (\n",
    "                torch.distributions.Normal(\n",
    "                    torch.zeros_like(res[\"x\"]), np.sqrt(sigma_eps)\n",
    "                )\n",
    "                .sample()\n",
    "                .to(device)\n",
    "            )\n",
    "            if torch.norm(eps) > norm_max:\n",
    "                epsilons = eps\n",
    "                norm_max = torch.norm(eps)\n",
    "        res[\"eps\"] = epsilons\n",
    "        scale_mean, scale_std = res[\"x\"].mean(0), res[\"x\"].std(0)\n",
    "        res[\"scale_parameters\"] = scale_mean, scale_std\n",
    "        res[\"scaled_x\"] = (res[\"x\"] - scale_mean) / scale_std\n",
    "        return res\n",
    "\n",
    "    def true_post(self, x_star: Tensor, sigma_2_y: float = None) -> Tuple:\n",
    "        \"\"\"Get the true posterior given an observation x_star.\n",
    "\n",
    "        Args:\n",
    "            x_star: An observation.\n",
    "            sigma_2_y (optional): Corruption. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            post: Posterior density evaluated from -8 to 8.\n",
    "        \"\"\"\n",
    "        if sigma_2_y is None:\n",
    "            sigma_2_y = self.sigma_y_2\n",
    "        theta_grid_t = np.linspace(-8, 8, 1000)\n",
    "        post = posterior_summaries(theta_grid_t, x_star[0], x_star[1], sigma_2_y)\n",
    "        return post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_2_y = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nsf(features, context):\n",
    "    \"\"\"Callable to instantiate the NPE with NSFs\"\"\"\n",
    "    return zuko.flows.NSF(features, context, bins=10, transforms=5).to(device)\n",
    "\n",
    "\n",
    "def train_flow(\n",
    "    flow: lampe.inference.NPE,\n",
    "    loss: Callable[[Tensor, Tensor], float],\n",
    "    theta: Tensor,\n",
    "    x: Tensor,\n",
    "    theta_test: Tensor,\n",
    "    x_test: Tensor,\n",
    ") -> lampe.inference.NPE:\n",
    "    \"\"\"Training procedure for the instantiated NPE.\n",
    "\n",
    "    Args:\n",
    "        flow (lampe.inference.NPE): Flow object to be trained.\n",
    "        loss (Callable[[Tensor, Tensor], float]): Loss function used for training.\n",
    "        theta (Tensor): Parameters.\n",
    "        x (Tensor): Observations.\n",
    "        theta_test (Tensor): Parameters used for early stopping.\n",
    "        x_test (Tensor): Observations used for early stopping.\n",
    "\n",
    "    Returns:\n",
    "        lampe.inference.NPE: The trained flow.\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(flow.parameters(), lr=5e-3)\n",
    "    # theta_test = theta_test.unsqueeze(-1).to(device)\n",
    "    data = lampe.data.JointDataset(theta.to(device), x.to(device))\n",
    "    loader = lampe.data.DataLoader(data, batch_size=256)\n",
    "    with torch.no_grad():\n",
    "        min_loss = loss(theta_test.to(device), x_test.to(device))\n",
    "    min_loss_list = [min_loss.item()]  # Convert min_loss to a scalar\n",
    "\n",
    "    flow.train()\n",
    "\n",
    "    for epoch in range(50):\n",
    "        for theta_batch, x_batch in loader:\n",
    "            theta_batch = theta_batch  # .unsqueeze(-1).to(device)\n",
    "            x_batch = x_batch.to(device)\n",
    "            losses = loss(theta_batch, x_batch)\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Checking for early stopping\n",
    "        with torch.no_grad():\n",
    "            loss_test = loss(theta_test.to(device), x_test.to(device))\n",
    "            min_loss_list.append(loss_test.item())  # Convert loss_test to a scalar\n",
    "            if (\n",
    "                len(min_loss_list) - np.argmin(min_loss_list) > 5\n",
    "            ):  # No improvement in loss(test) for the last 5 iterations\n",
    "                # Early stop\n",
    "                break\n",
    "    flow.eval()\n",
    "    return flow\n",
    "\n",
    "\n",
    "def create_train_flow(task) -> lampe.inference.NPE:\n",
    "    \"\"\"Creates a conditional flow (NSF) and trains it on the task data.\n",
    "\n",
    "    Args:\n",
    "        task: A task object.\n",
    "\n",
    "    Returns:\n",
    "        lampe.inference.NPE: The trained flow.\n",
    "    \"\"\"\n",
    "    data = task.data\n",
    "    data_test = task.data_test\n",
    "    theta = data[\"theta\"]\n",
    "    theta_test = data_test[\"theta\"]\n",
    "    flow = lampe.inference.NPE(\n",
    "        theta_dim=task.parameters_dim, x_dim=task.features_dim, build=build_nsf\n",
    "    ).to(device)\n",
    "    loss = lampe.inference.NPELoss(flow).to(device)\n",
    "    x = data[\"scaled_x\"].to(device)\n",
    "    x_test = data_test[\"scaled_x\"].to(device)\n",
    "    flow = train_flow(flow, loss, theta, x, theta_test, x_test)\n",
    "    return flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unconditional_flow(\n",
    "    flow: zuko.flows, loss: Callable[[Tensor], float], x: Tensor, x_test: Tensor\n",
    ") -> zuko.flows:\n",
    "    \"\"\"Trains a unconditional flow on x.\n",
    "\n",
    "    Args:\n",
    "        flow: Object flow to be trained.\n",
    "        loss: Method to compute a loss.\n",
    "        x: Training data.\n",
    "        x_test: Test data, used for early stopping.\n",
    "\n",
    "    Returns:\n",
    "        flow: Trained flow object.\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(flow.parameters(), lr=1e-2)\n",
    "    loader = torch.utils.data.DataLoader(x.to(device), 256)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        min_loss = loss(x_test.to(device))\n",
    "    min_loss_list = [min_loss.item()]  # Convert min_loss to a scalar\n",
    "\n",
    "    flow.train()\n",
    "\n",
    "    for epoch in range(50):\n",
    "        for x_batch in loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            losses = loss(x_batch)\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Checking for early stopping\n",
    "        with torch.no_grad():\n",
    "            loss_test = loss(x_test.to(device))\n",
    "            min_loss_list.append(loss_test.item())  # Convert loss_test to a scalar\n",
    "            if (\n",
    "                len(min_loss_list) - np.argmin(min_loss_list) > 5\n",
    "            ):  # No improvement in loss(test) for the last 5 iterations\n",
    "                # Early stop\n",
    "\n",
    "                break\n",
    "    flow.eval()\n",
    "    return flow\n",
    "\n",
    "\n",
    "def create_train_unconditional_flow(task) -> zuko.flows:\n",
    "    \"\"\"Instanciate and trains an unconditional flow (MAF) on the task data.\n",
    "\n",
    "    Args:\n",
    "        task: A task, containing train and test data.\n",
    "\n",
    "    Returns:\n",
    "        flow: The trained flow.\n",
    "    \"\"\"\n",
    "    data = task.data\n",
    "    data_test = task.data_test\n",
    "    flow = zuko.flows.NAF(features=task.features_dim, context=0).to(device)  #!\n",
    "    loss = lambda x: -flow().log_prob(x).mean()\n",
    "    x = data[\"scaled_x\"].to(device)\n",
    "    x_test = data_test[\"scaled_x\"].to(device)\n",
    "    flow = train_unconditional_flow(flow, loss, x, x_test)\n",
    "    return flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMCMC:\n",
    "    r\"\"\"Class to sample from the posterior :math: `p(\\theta \\mid y)`\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, y0: Tensor, tau: float, sigma: float, rho: float, q_x_NF: zuko.flows\n",
    "    ) -> None:\n",
    "        \"\"\"Init\n",
    "\n",
    "        Args:\n",
    "            y0: Observation.\n",
    "            tau: Parameter of the slab (Cauchy) distribution.\n",
    "            sigma: Parameter (variance) of the spike (Normal) distribution.\n",
    "            rho: Parameter of the Bernoulli distribution.\n",
    "            q_x_NF: Unconditional flow.\n",
    "        \"\"\"\n",
    "        self.y0 = y0\n",
    "        self.tau = tau\n",
    "        self.sigma = sigma\n",
    "        self.rho = rho\n",
    "        self.q_x_NF = q_x_NF\n",
    "\n",
    "    def f_density(self, y0: Tensor, x: Tensor) -> Tensor:\n",
    "        \"\"\"Error model\n",
    "\n",
    "        Args:\n",
    "            y0: Observation.\n",
    "            x: x proposed for the sampler. (can be batched)\n",
    "\n",
    "        Returns:\n",
    "            res: Density evaluated at x (can be batched)\n",
    "        \"\"\"\n",
    "        D = x.shape[-1]  # context dimension\n",
    "        res = torch.zeros_like(x)\n",
    "        for j in range(D):\n",
    "            xj = x[:, j]\n",
    "            yj = y0[j]  # TODO\n",
    "            zj = (\n",
    "                torch.distributions.Bernoulli(rho).sample().to(device)\n",
    "            )  # Whetehr the model is miss specified\n",
    "            if not zj:\n",
    "                spike_dist = torch.distributions.Normal(\n",
    "                    xj.detach(), torch.tensor(sigma).to(device)\n",
    "                )\n",
    "                res[:, j] = torch.exp(spike_dist.log_prob(yj))\n",
    "            else:\n",
    "                slab_dist = torch.distributions.Cauchy(\n",
    "                    xj.detach(), torch.tensor(tau).to(device)\n",
    "                )\n",
    "                res[:, j] = torch.exp(slab_dist.log_prob(yj))\n",
    "        return res  # * torch.exp(q_x_NF().log_prob(x))[:,None]\n",
    "\n",
    "    def proposal(\n",
    "        self, M: int, warm_up_steps: int, proposal_data: Tensor, n_chains\n",
    "    ) -> Tensor:\n",
    "        \"\"\"Transition kernel. Samples from the unconditional kernel.\n",
    "\n",
    "        Args:\n",
    "            M: Total number of sample.\n",
    "            warm_up_steps: Burn-in period.\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            Proposed sample.\n",
    "        \"\"\"\n",
    "        idx = torch.randint(M + warm_up_steps, (n_chains,))\n",
    "        return proposal_data[idx]\n",
    "\n",
    "    def sample(self, M: int, warm_up_steps: int, features_dim: int) -> Tensor:\n",
    "        f_lampe = lambda x: self.f_density(self.y0, x)\n",
    "        n_chains = 8  # TODO\n",
    "        x_curr = torch.FloatTensor(n_chains, features_dim).uniform_(-1, 1).to(device)\n",
    "        my_samples = torch.empty(\n",
    "            (n_chains, warm_up_steps + M // n_chains, x_curr.shape[1])\n",
    "        ).to(device)\n",
    "        log_f_x_curr = f_lampe(x_curr).log()\n",
    "\n",
    "        proposal_data = self.q_x_NF().sample((M + warm_up_steps,))\n",
    "        with torch.no_grad():\n",
    "            for i in range(int(warm_up_steps + M // n_chains)):\n",
    "                x_star = self.proposal(M, warm_up_steps, proposal_data, n_chains)\n",
    "                log_f_x_star = f_lampe(x_star).log()\n",
    "                log_a = log_f_x_star - log_f_x_curr\n",
    "                a = torch.exp(log_a)\n",
    "                u = torch.FloatTensor(a.shape).uniform_().to(device)\n",
    "                mask = u < a\n",
    "                x_curr = torch.where(mask, x_star, x_curr)\n",
    "                log_f_x_curr = torch.where(mask, log_f_x_star, log_f_x_curr)\n",
    "                my_samples[:, i] = x_curr\n",
    "            my_samples = my_samples[:, warm_up_steps:, :]\n",
    "            samples_mcmc = my_samples  # [:,:M//n_chains,:]\n",
    "\n",
    "            samples_mcmc_ = samples_mcmc.reshape(1, M, x_curr.shape[-1]).squeeze()\n",
    "        return samples_mcmc_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to check if it works, this is the way I intended this to be used:\n",
    "task = Gaussian(N_simu=N_simu, sigma_y_2=sigma_2_y)\n",
    "q_NPE = create_train_flow(task)\n",
    "q_x_NF = create_train_unconditional_flow(task)\n",
    "sampler = MyMCMC(task.y0_scaled.squeeze(), tau, sigma, rho, q_x_NF)\n",
    "xm = sampler.sample(M, warm_up_steps, task.features_dim)\n",
    "\n",
    "rnpe_samples = q_NPE.flow(xm).sample()\n",
    "true_post_y = posterior_summaries(\n",
    "    theta_grid,\n",
    "    task.y0.cpu().squeeze()[0].item(),\n",
    "    task.y0.cpu().squeeze()[1].item(),\n",
    "    sigma_2_y,\n",
    ")\n",
    "npe_samples = q_NPE.flow(task.y0_scaled).sample((20_000,))\n",
    "true_post = task.true_post(task.y0.squeeze().cpu().numpy(), sigma_2_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 8))\n",
    "\n",
    "\n",
    "ax1.scatter(\n",
    "    x=task.data[\"scaled_x\"][:, 0].cpu(),\n",
    "    y=task.data[\"scaled_x\"][:, 1].cpu(),\n",
    "    color=\"blue\",\n",
    "    label=\"Simulator outputs\",\n",
    "    s=1,\n",
    ")\n",
    "ax1.scatter(\n",
    "    x=xm[:, 0].cpu(),\n",
    "    y=xm[:, 1].cpu(),\n",
    "    label=\"Denoised outputs from the MCMC sampler\",\n",
    "    color=\"yellow\",\n",
    "    s=1,\n",
    ")\n",
    "ax1.scatter(\n",
    "    x=task.y0_scaled.cpu()[0],\n",
    "    y=task.y0_scaled.cpu()[1],\n",
    "    label=\"y0 scaled\",\n",
    "    color=\"green\",\n",
    "    s=20,\n",
    ")\n",
    "ax1.set_xlabel(r\"$mean (z_1,...,z_n)$\")\n",
    "ax1.set_ylabel(r\"$Var(z_1,...,z_n)$\")\n",
    "ax1.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n",
    "\n",
    "\n",
    "ax2.hist(\n",
    "    npe_samples.squeeze().cpu(),\n",
    "    bins=200,\n",
    "    density=True,\n",
    "    color=\"green\",\n",
    "    alpha=0.5,\n",
    "    label=r\"NPE samples $q_{\\Phi}(\\theta|y_0)$\",\n",
    ")\n",
    "ax2.hist(\n",
    "    rnpe_samples.squeeze().cpu(),\n",
    "    bins=200,\n",
    "    density=True,\n",
    "    color=\"red\",\n",
    "    alpha=0.5,\n",
    "    label=r\"RNPE samples $q_{\\Phi}(\\theta|y_0)$\",\n",
    ")\n",
    "ax2.plot(\n",
    "    theta_grid,\n",
    "    true_post_y,\n",
    "    color=\"yellow\",\n",
    "    label=r\" $p(\\theta|y_0)$ under the true DGP\",\n",
    ")\n",
    "\n",
    "ax2.legend()\n",
    "\n",
    "ax2.set_xlim(-5, 5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portion de code repris de Schmitt et. al.\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def kl_latent_space(z, log_det_J):\n",
    "    \"\"\"Computes the Kullback-Leibler divergence (Maximum Likelihood Loss) between true and approximate\n",
    "    posterior using simulated data and parameters.\n",
    "    \"\"\"\n",
    "    loss = torch.mean(0.5 * torch.norm(z, dim=-1) ** 2 - log_det_J)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def maximum_mean_discrepancy(\n",
    "    source_samples,\n",
    "    target_samples,\n",
    "    kernel=\"gaussian\",\n",
    "    minimum=0.0,\n",
    "    unbiased=False,\n",
    "    squared=True,\n",
    "):\n",
    "    \"\"\"This Maximum Mean Discrepancy (MMD) loss is calculated with a number of different Gaussian or Inverse-Multiquadratic kernels.\"\"\"\n",
    "    sigmas = torch.tensor(\n",
    "        [  # Convert list to tensor\n",
    "            1e-3,  #!\n",
    "        ],\n",
    "        dtype=torch.float32,\n",
    "        device=source_samples.device,\n",
    "    )\n",
    "\n",
    "    if kernel == \"gaussian\":\n",
    "        kernel = partial(_gaussian_kernel_matrix, sigmas=sigmas)\n",
    "    elif kernel == \"inverse_multiquadratic\":\n",
    "        kernel = partial(_inverse_multiquadratic_kernel_matrix, sigmas=sigmas)\n",
    "    else:\n",
    "        print(\"Invalid kernel specified. Falling back to default Gaussian.\")\n",
    "        kernel = partial(_gaussian_kernel_matrix, sigmas=sigmas)\n",
    "\n",
    "    if unbiased:\n",
    "        loss_value = _mmd_kernel_unbiased(source_samples, target_samples, kernel=kernel)\n",
    "    else:\n",
    "        loss_value = _mmd_kernel(source_samples, target_samples, kernel=kernel)\n",
    "\n",
    "    loss_value = max(minimum, loss_value)\n",
    "\n",
    "    if squared:\n",
    "        return loss_value\n",
    "    else:\n",
    "        return torch.sqrt(loss_value)\n",
    "\n",
    "\n",
    "def _gaussian_kernel_matrix(x, y, sigmas):\n",
    "    norm = lambda v: torch.sum(v**2, dim=2)  # Update dimension to 2\n",
    "    beta = 1.0 / (2.0 * (sigmas[:, None]))\n",
    "    dist = norm(x[:, None, :] - y[None, :, :])  # Remove transpose\n",
    "    s = torch.matmul(beta, torch.reshape(dist, (1, -1)))\n",
    "    kernel = torch.reshape(torch.sum(torch.exp(-s), 0), dist.shape)\n",
    "    return kernel\n",
    "\n",
    "\n",
    "def _inverse_multiquadratic_kernel_matrix(x, y, sigmas):\n",
    "    \"\"\"Computes an inverse multiquadratic RBF between the samples of x and y.\n",
    "    We create a sum of multiple IM-RBF kernels each having a width sigma_i.\n",
    "    \"\"\"\n",
    "    dist = torch.unsqueeze(\n",
    "        torch.sum((x[:, None, :] - y[None, :, :]) ** 2, dim=-1), dim=-1\n",
    "    )\n",
    "    sigmas = torch.unsqueeze(sigmas, dim=0)\n",
    "    return torch.sum(sigmas / (dist + sigmas), dim=-1)\n",
    "\n",
    "\n",
    "def _mmd_kernel(x, y, kernel=None):\n",
    "    \"\"\"Computes the Maximum Mean Discrepancy (MMD) of two samples: x and y.\n",
    "    Maximum Mean Discrepancy (MMD) is a distance-measure between the samples of the distributions of x and y.\n",
    "    \"\"\"\n",
    "    loss = torch.mean(kernel(x, x))\n",
    "    loss += torch.mean(kernel(y, y))\n",
    "    loss -= 2 * torch.mean(kernel(x, y))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def _mmd_kernel_unbiased(x, y, kernel=None):\n",
    "    \"\"\"Computes the Maximum Mean Discrepancy (MMD) of two samples: x and y.\n",
    "    Maximum Mean Discrepancy (MMD) is a distance-measure between the samples of the distributions of x and y.\n",
    "    \"\"\"\n",
    "    m, n = x.shape[0], y.shape[0]\n",
    "    print(kernel(x, x))\n",
    "    loss = (1.0 / (m * (m - 1))) * torch.sum(kernel(x, x))\n",
    "    loss += (1.0 / (n * (n - 1))) * torch.sum(kernel(y, y))\n",
    "    loss -= (2.0 / (m * n)) * torch.sum(kernel(x, y))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def MMD_bootstrap(\n",
    "    x, x_o, N_BOOTSTRAP_ITERATIONS=10, n_samples_x=500, n_samples_x_o=500\n",
    "):\n",
    "    n_x = x.shape[0]\n",
    "    n_x_o = x_o.shape[0]\n",
    "\n",
    "    MMD_bootstrap = np.empty(N_BOOTSTRAP_ITERATIONS)\n",
    "    for i in tqdm(range(N_BOOTSTRAP_ITERATIONS)):\n",
    "        idx_x = np.random.randint(0, n_x, size=n_samples_x)\n",
    "        idx_x_o = np.random.randint(0, n_x_o, size=n_samples_x_o)\n",
    "\n",
    "        x_bootstrap = x[idx_x]\n",
    "        x_o_bootstrap = x_o[idx_x_o]\n",
    "\n",
    "        MMD_bootstrap[i] = float(\n",
    "            maximum_mean_discrepancy(x_bootstrap, x_o_bootstrap, squared=False)\n",
    "        )\n",
    "    return MMD_bootstrap\n",
    "\n",
    "\n",
    "def calculate_CI(x, ci_area=0.95):\n",
    "    q_lower = round((1.0 - ci_area) / 2, 5)\n",
    "    q_upper = round(1.0 - q_lower, 5)\n",
    "    return np.quantile(x, q_lower), np.quantile(x, q_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that is what I did at first, I computed the estimated density of thetz using NPE & RNPE for 10 runs (the two following functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rnpe_npe(\n",
    "    task_name: str, sigma_2: float\n",
    ") -> Tuple[Tensor, Tensor, Tuple, Tensor]:\n",
    "    \"\"\"General pipeline to compute both NPE and RNPE samples for a given task and corruption.\n",
    "\n",
    "    Args:\n",
    "        task_name: Name of the task.\n",
    "        sigma_2: Corruption.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Tensor, Tensor, Tuple, Tensor]: NPE samples given x, y, theroritical given y and RNPE samples given y.\n",
    "    \"\"\"\n",
    "\n",
    "    n_runs = 10\n",
    "    task_l = []\n",
    "    xm_l = []\n",
    "    norm_npe_rnpe = []\n",
    "    npe_samples_y_l = []\n",
    "    theoritical_post_y_l = []\n",
    "    rnpe_samples_l = []\n",
    "\n",
    "    for _ in tqdm(range(n_runs), leave=True):\n",
    "        task = (\n",
    "            Gaussian(N_simu=N_simu, sigma_y_2=sigma_2)\n",
    "            if task_name == \"Gaussian\"\n",
    "            else CS(N_simu=N_simu, miss_specification_param=sigma_2)\n",
    "        )\n",
    "        q_NPE = create_train_flow(task)\n",
    "        q_x_NF = create_train_unconditional_flow(task)\n",
    "        with torch.no_grad():\n",
    "            mcmc_sampler = MyMCMC(task.y0_scaled.squeeze(), tau, sigma, rho, q_x_NF)\n",
    "            xm = mcmc_sampler.sample(M, warm_up_steps, task.features_dim)\n",
    "        npe_samples_y = q_NPE.flow(task.y0_scaled).sample((M,))\n",
    "        theoritical_post_y = (\n",
    "            task.true_post(task.y0.squeeze().cpu().numpy(), sigma_2)\n",
    "            if task.has_post\n",
    "            else []\n",
    "        )\n",
    "        rnpe_samples = q_NPE.flow(xm).sample()\n",
    "        norm_npe_rnpe.append(npe_samples_y.mean() - rnpe_samples.mean())\n",
    "\n",
    "        task_l.append(task)\n",
    "        xm_l.append(xm)\n",
    "        npe_samples_y_l.append(npe_samples_y)\n",
    "        theoritical_post_y_l.append(theoritical_post_y)\n",
    "        rnpe_samples_l.append(rnpe_samples)\n",
    "    return {\n",
    "        \"task_l\": task_l,\n",
    "        \"npe_samples_y_l\": npe_samples_y_l,\n",
    "        \"theoritical_post_y_l\": theoritical_post_y_l,\n",
    "        \"rnpe_samples_l\": rnpe_samples_l,\n",
    "        \"xm_l\": xm_l,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(\n",
    "    task_l: List,\n",
    "    npe_samples_y_l: List,\n",
    "    theoritical_post_y_l: List,\n",
    "    rnpe_samples_l: List,\n",
    "    xm_l: List,\n",
    ") -> Dict:\n",
    "    npe_samples_y, theoritical_post_y, rnpe_samples, task, MMD_npe, MMD_rnpe = (\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "    )\n",
    "    # Computiing MMD on each run\n",
    "    MMD_npe = np.zeros((1, len(xm_l)))\n",
    "    MMD_rnpe = np.zeros((1, len(xm_l)))\n",
    "    norm_npe_rnpe = []\n",
    "    for i in range(len(xm_l)):\n",
    "        # Rescaling the xm\n",
    "        xm_rescaled = (xm_l[i] + task_l[i].data[\"scale_parameters\"][0]) * task_l[\n",
    "            i\n",
    "        ].data[\"scale_parameters\"][1]\n",
    "\n",
    "        # MMD[1, j] = float(maximum_mean_discrepancy(task_l[i].data[\"x\"], task_l[i].y0.expand(40_000, -1), squared=False))\n",
    "        idx_npe = np.random.randint(0, task_l[i].data[\"scaled_x\"].shape[0], size=1_000)\n",
    "        MMD_npe[0, i] = float(\n",
    "            maximum_mean_discrepancy(\n",
    "                task_l[i].y0_scaled.expand(10_000, -1),\n",
    "                task_l[i].data[\"scaled_x\"][idx_npe],\n",
    "                squared=False,\n",
    "            )\n",
    "        )\n",
    "        idx_rnpe = np.random.randint(0, xm_l[i].shape[0], size=1_000)\n",
    "        MMD_rnpe[0, i] = float(\n",
    "            maximum_mean_discrepancy(\n",
    "                task_l[i].y0_scaled.expand(10_000, -1), xm_l[i][idx_rnpe], squared=False\n",
    "            )\n",
    "        )\n",
    "        norm_npe_rnpe.append(\n",
    "            np.square(npe_samples_y_l[i].mean() - rnpe_samples_l[i].mean())\n",
    "            + np.square(np.log(npe_samples_y_l[i].std() / rnpe_samples_l[i].std()))\n",
    "        )\n",
    "\n",
    "    imax = np.argmax(norm_npe_rnpe)\n",
    "    npe_samples_y = npe_samples_y_l[imax]\n",
    "    theoritical_post_y = theoritical_post_y_l[imax]\n",
    "    rnpe_samples = rnpe_samples_l[imax]\n",
    "    task = task_l[imax]\n",
    "\n",
    "    return {\n",
    "        \"npe_samples_y\": npe_samples_y,\n",
    "        \"theoritical_post_y\": theoritical_post_y,\n",
    "        \"rnpe_samples\": rnpe_samples,\n",
    "        \"task\": task,\n",
    "        \"MMD_npe\": MMD_npe,\n",
    "        \"MMD_rnpe\": MMD_rnpe,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can plot the estimated densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "sigmas_2 = [1, 1.25, 1.5, 1.75, 2, 2.25, 2.5, 2.75, 3]\n",
    "with tqdm(sigmas_2, unit=\"Sigma²\", desc=\"Corruption\") as tq:\n",
    "    for sigma_2 in tq:\n",
    "        print(sigma_2)\n",
    "        donnees = compute_rnpe_npe(\"Gaussian\", sigma_2)\n",
    "        results[sigma_2] = compute_metrics(**donnees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@widgets.interact(sigma_2=results.keys())  # Seulement pour tâche gaussienne...\n",
    "def plot_posteriors_widget(sigma_2=1):\n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(\n",
    "        results[sigma_2][\"npe_samples_y\"].squeeze(),\n",
    "        bins=400,\n",
    "        color=\"green\",\n",
    "        density=True,\n",
    "        label=\"Posterior samples from NPE\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    ax.hist(\n",
    "        results[sigma_2][\"rnpe_samples\"].squeeze(),\n",
    "        bins=400,\n",
    "        color=\"red\",\n",
    "        density=True,\n",
    "        label=\"Posterior samples from RNPE\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    ax.plot(theta_grid, results[sigma_2][\"theoritical_post_y\"])\n",
    "    ax.set_xlim(-5, 5)\n",
    "    ax.legend()\n",
    "    ax.set_title(f\"NPE and RNPE samples, sigma_y² = {sigma_2}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[1, 1.25, 1.5, 1.75, 2, 2.25, 2.5, 2.75, 3]\n",
    "\n",
    "\n",
    "def create_compute(filename, task_name, miss_specification_params):\n",
    "    results = {}\n",
    "    n_sim = 100\n",
    "    n_runs_per_pi = 10\n",
    "    MMD_npe = np.zeros((len(miss_specification_params), n_runs_per_pi))\n",
    "    MMD_rnpe = np.zeros((len(miss_specification_params), n_runs_per_pi))\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    for i, pi in tqdm(enumerate(miss_specification_params)):\n",
    "        donnees = {}\n",
    "        results = {}\n",
    "        for j in range(n_runs_per_pi):\n",
    "            task = (\n",
    "                Gaussian(N_simu=N_simu, sigma_y_2=pi)\n",
    "                if task_name == \"Gaussian\"\n",
    "                else CS(N_simu=N_simu, miss_specification_param=pi)\n",
    "            )\n",
    "            q_NPE = create_train_flow(task)\n",
    "            q_x_NF = create_train_unconditional_flow(task)\n",
    "            with torch.no_grad():\n",
    "                mcmc_sampler = MyMCMC(task.y0_scaled.squeeze(), tau, sigma, rho, q_x_NF)\n",
    "                xm = mcmc_sampler.sample(M, warm_up_steps, task.features_dim)\n",
    "            npe_samples_y = q_NPE.flow(task.y0_scaled).sample((M,))\n",
    "            # theoritical_post_y = task.true_post(task.y0.squeeze().cpu().numpy(), sigma_2) if task.has_post else []\n",
    "            rnpe_samples = q_NPE.flow(xm).sample()\n",
    "\n",
    "            n_npe = task.data[\"x\"].shape[0]\n",
    "            n_rnpe = xm.shape[0]\n",
    "            n_y0 = task.y0_l.shape[0]\n",
    "            idx_npe = np.random.randint(0, n_npe, size=1_000)\n",
    "            idx_rnpe = np.random.randint(0, n_rnpe, size=1_000)\n",
    "            idx_y0 = np.random.randint(0, n_y0, size=1_000)\n",
    "            MMD_npe[i, j] = float(\n",
    "                maximum_mean_discrepancy(\n",
    "                    task.y0_scaled.expand(10_000, -1),\n",
    "                    task.data[\"scaled_x\"][idx_npe],\n",
    "                    squared=False,\n",
    "                )\n",
    "            )\n",
    "            MMD_rnpe[i, j] = float(\n",
    "                maximum_mean_discrepancy(\n",
    "                    task.y0_scaled.expand(10_000, -1), xm[idx_rnpe], squared=False\n",
    "                )\n",
    "            )\n",
    "        results[\"mmd_npe\"] = MMD_npe\n",
    "        results[\"mmd_rnpe\"] = MMD_rnpe\n",
    "        with open(filename + task.name + str(pi), \"wb\") as f:\n",
    "            pickle.dump(results, f)\n",
    "\n",
    "    return MMD_npe, MMD_rnpe\n",
    "\n",
    "\n",
    "create_compute(\"./cache/\", \"Gaussian\", sigmas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "sigmas_2 = [1, 1.25, 1.5, 1.75, 2, 2.25, 2.5, 2.75, 3]\n",
    "for sigma_2 in sigmas_2:\n",
    "    with open(\"./cache/Gaussian\" + str(sigma_2), \"rb\") as f:\n",
    "        results[sigma_2] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sigma_2 in sigmas_2:\n",
    "    plt.scatter(sigma_2, results[sigma_2][\"mmd_rnpe\"].mean(), c=\"r\")\n",
    "    plt.scatter(sigma_2, results[sigma_2][\"mmd_npe\"].mean(), c=\"b\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi_env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
